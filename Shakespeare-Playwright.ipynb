{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Shakespeare Playwright.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTLEUIbO5DYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7cJvgtG5DZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bd6cc3df-c739-4f57-baed-32b015b9d5d5"
      },
      "source": [
        "path = tf.keras.utils.get_file('shakespeare.txt',\n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orCa6MSc5DZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "e05c905e-c4eb-42f5-dfa6-6e7d50988d99"
      },
      "source": [
        "print(\"Length of text {} characters\" .format(len(text)))\n",
        "print(text[:250])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WBpAnHY5DaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba119b3b-41d9-44cd-dcf1-ec2cb7a22c37"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc4m_GoM5Daq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:idx for idx, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "textAsInt = np.array([char2idx[char] for char in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmD3kY7-5Da0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "bb3221a6-73c1-4023-bdb3-c4b582404542"
      },
      "source": [
        "print(\"{\")\n",
        "for char, _ in zip(char2idx, range(20)):\n",
        "    print(\"    {:4s}: {:3d}\".format(repr(char), char2idx[char]))\n",
        "print(\"}\\n....\\n\")\n",
        "\n",
        "print('{}  ------> characters mapped to int -----> {}'.format(repr(text[:13]),\n",
        "                                                             textAsInt[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    '\\n':   0\n",
            "    ' ' :   1\n",
            "    '!' :   2\n",
            "    '$' :   3\n",
            "    '&' :   4\n",
            "    \"'\" :   5\n",
            "    ',' :   6\n",
            "    '-' :   7\n",
            "    '.' :   8\n",
            "    '3' :   9\n",
            "    ':' :  10\n",
            "    ';' :  11\n",
            "    '?' :  12\n",
            "    'A' :  13\n",
            "    'B' :  14\n",
            "    'C' :  15\n",
            "    'D' :  16\n",
            "    'E' :  17\n",
            "    'F' :  18\n",
            "    'G' :  19\n",
            "}\n",
            "....\n",
            "\n",
            "'First Citizen'  ------> characters mapped to int -----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBMZZMRF5DbT",
        "colab_type": "text"
      },
      "source": [
        "# Readying Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN2aN-Ic5DbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seqLength = 100\n",
        "examplesPerEpoch = len(text) // (seqLength + 1)\n",
        "\n",
        "charDataset = tf.data.Dataset.from_tensor_slices(textAsInt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5xmeHWH5Dbm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f0ceceac-bc5d-499b-b856-9a6d517925d1"
      },
      "source": [
        "for i in charDataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbUiwQxY5DcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = charDataset.batch(seqLength + 1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tMRXn345Dcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "28654727-9e9e-455f-aea8-de3f848d615b"
      },
      "source": [
        "for item in sequences.take(5):\n",
        "    print(repr(\"\".join(idx2char[item.numpy()])))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjD8LV4V5Dcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitInputTarget(chunk):\n",
        "    inputText = chunk[:-1]\n",
        "    targetText = chunk[1:]\n",
        "    return inputText, targetText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFftVo_55DdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sequences.map(splitInputTarget)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jip4zf0Q5DdW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "32b193ec-06c0-4baf-aa5a-961d0b181e76"
      },
      "source": [
        "for inputEx, targetEx in dataset.take(1):\n",
        "    print(\"Input data: \", repr(\"\".join(idx2char[inputEx.numpy()])))\n",
        "    print(\"Target data: \", repr(\"\".join(idx2char[targetEx.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHYYrvaT5Ddk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "8e8b1c94-6e3b-44e4-97b4-93d9da145bd1"
      },
      "source": [
        "for i, (inputIdx, targetIdx) in enumerate(zip(inputEx[:5], targetEx[:5])):\n",
        "    print(\"step {:4d}\".format(i))\n",
        "    print(\"  input {} ({:s})\".format(inputIdx, repr(idx2char[inputIdx])))\n",
        "    print(\"  expected output {} ({:s})\".format(targetIdx, repr(idx2char[targetIdx])))\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step    0\n",
            "  input 18 ('F')\n",
            "  expected output 47 ('i')\n",
            "step    1\n",
            "  input 47 ('i')\n",
            "  expected output 56 ('r')\n",
            "step    2\n",
            "  input 56 ('r')\n",
            "  expected output 57 ('s')\n",
            "step    3\n",
            "  input 57 ('s')\n",
            "  expected output 58 ('t')\n",
            "step    4\n",
            "  input 58 ('t')\n",
            "  expected output 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOIZ1WVL5Ddy",
        "colab_type": "text"
      },
      "source": [
        "Creating Training Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6qtbO2B5Dd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea56c9d3-512f-4b1d-f5dd-0ef22d4265d9"
      },
      "source": [
        "batchSize = 64\n",
        "bufSize = 10000\n",
        "\n",
        "dataset = dataset.shuffle(bufSize).batch(batchSize, drop_remainder=True)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T_qM19f5DeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabSize = len(vocab)\n",
        "embeddingDim = 256\n",
        "rnnUnits = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2slNiKWZ5Deq",
        "colab_type": "text"
      },
      "source": [
        "# BUILDING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDqdwITj5Des",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildModel(vocabSize, embeddingDim, rnnUnits, batchSize):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocabSize, embeddingDim, batch_input_shape=[batchSize, None]),\n",
        "        tf.keras.layers.GRU(rnnUnits, return_sequences=True, stateful=True,\n",
        "                           recurrent_initializer=\"glorot_uniform\"),\n",
        "        tf.keras.layers.Dense(vocabSize)\n",
        "    ])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c71CKqx5DfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = buildModel(vocabSize=len(vocab), embeddingDim =embeddingDim, rnnUnits=rnnUnits, batchSize=batchSize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y37UNDX55DfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a46e314b-69a1-4eb7-ea35-a3177b1c02d0"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4_deZc7f5Dfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "a0bf3aa4-5f78-438d-b446-87cac19e4146"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTCThs0j5DgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9349b11b-81b8-422a-8d70-b3ecfe442726"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "print(sampled_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[28 54 16  1  9 53 51  0  4 64 64 45 10 58 27 45 46 54 45 32 11 21 57 39\n",
            " 46 62 46 45 23 22 63 14  5 48 35 54 38 64 23  6 57 48 13 23 22 22 61 62\n",
            "  4  9 36 54 15 24 29 28 49 37 56 48 40 61 61 44 41 19 44 58 57 39 61 18\n",
            " 31  8  6 59 11 44 53 13 47 23 39 16  3 42 22 43 41 48 35 15 17 37 38 17\n",
            " 52 57 30 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1-OEFyv5DgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1b29b89c-f5b8-419a-c1f2-d2b7aa510c69"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'readiest way to make the wench amends\\nIs to become her husband and her father:\\nThe which will I; not'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"PpD 3om\\n&zzg:tOghpgT;IsahxhgKJyB'jWpZzK,sjAKJJwx&3XpCLQPkYrjbwwfcGftsawFS.,u;foAiKaD$dJecjWCEYZEnsRG\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1JzxWH75Dgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, \n",
        "                                                           logits, \n",
        "                                                           from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bREWH-s5Dgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1e17e652-aca1-4b13-d391-7e77851e460d"
      },
      "source": [
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.174753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GfiU10D5Dg3",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLkSHpzH5Dg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vQRUvrk5DhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKVDjWsl5DhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJilSTfV5DhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96b7e200-eaf6-4ad0-edab-db16ab2e94d1"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - 23s 135ms/step - loss: 2.6442\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.9540\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.6843\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.5367\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.4510\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.3928\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.3470\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.3087\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.2757\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.2420\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.2098\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.1779\n",
            "Epoch 13/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.1454\n",
            "Epoch 14/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.1114\n",
            "Epoch 15/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.0784\n",
            "Epoch 16/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.0414\n",
            "Epoch 17/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 1.0074\n",
            "Epoch 18/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.9720\n",
            "Epoch 19/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.9379\n",
            "Epoch 20/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.9036\n",
            "Epoch 21/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8723\n",
            "Epoch 22/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8431\n",
            "Epoch 23/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.8174\n",
            "Epoch 24/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.7925\n",
            "Epoch 25/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.7713\n",
            "Epoch 26/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.7527\n",
            "Epoch 27/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7371\n",
            "Epoch 28/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7242\n",
            "Epoch 29/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7113\n",
            "Epoch 30/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7005\n",
            "Epoch 31/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6919\n",
            "Epoch 32/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6825\n",
            "Epoch 33/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6786\n",
            "Epoch 34/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6697\n",
            "Epoch 35/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6650\n",
            "Epoch 36/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6600\n",
            "Epoch 37/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6566\n",
            "Epoch 38/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6544\n",
            "Epoch 39/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6515\n",
            "Epoch 40/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6467\n",
            "Epoch 41/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6466\n",
            "Epoch 42/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6423\n",
            "Epoch 43/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6420\n",
            "Epoch 44/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6432\n",
            "Epoch 45/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6421\n",
            "Epoch 46/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6420\n",
            "Epoch 47/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6411\n",
            "Epoch 48/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6427\n",
            "Epoch 49/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6449\n",
            "Epoch 50/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6436\n",
            "Epoch 51/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6476\n",
            "Epoch 52/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6510\n",
            "Epoch 53/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6523\n",
            "Epoch 54/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6557\n",
            "Epoch 55/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6572\n",
            "Epoch 56/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6574\n",
            "Epoch 57/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6600\n",
            "Epoch 58/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6600\n",
            "Epoch 59/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6614\n",
            "Epoch 60/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6681\n",
            "Epoch 61/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6741\n",
            "Epoch 62/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6770\n",
            "Epoch 63/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6800\n",
            "Epoch 64/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6810\n",
            "Epoch 65/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.6861\n",
            "Epoch 66/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6947\n",
            "Epoch 67/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.6967\n",
            "Epoch 68/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7048\n",
            "Epoch 69/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7069\n",
            "Epoch 70/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7144\n",
            "Epoch 71/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.7176\n",
            "Epoch 72/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.7274\n",
            "Epoch 73/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.7416\n",
            "Epoch 74/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.7506\n",
            "Epoch 75/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7528\n",
            "Epoch 76/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7677\n",
            "Epoch 77/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7759\n",
            "Epoch 78/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.7909\n",
            "Epoch 79/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8003\n",
            "Epoch 80/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8159\n",
            "Epoch 81/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8224\n",
            "Epoch 82/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8319\n",
            "Epoch 83/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8437\n",
            "Epoch 84/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8653\n",
            "Epoch 85/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.8765\n",
            "Epoch 86/100\n",
            "172/172 [==============================] - 23s 134ms/step - loss: 0.8900\n",
            "Epoch 87/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.9016\n",
            "Epoch 88/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.9153\n",
            "Epoch 89/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.9312\n",
            "Epoch 90/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.9560\n",
            "Epoch 91/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.9694\n",
            "Epoch 92/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 0.9868\n",
            "Epoch 93/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.0090\n",
            "Epoch 94/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.0386\n",
            "Epoch 95/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.0466\n",
            "Epoch 96/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.0581\n",
            "Epoch 97/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.0699\n",
            "Epoch 98/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.0765\n",
            "Epoch 99/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.1059\n",
            "Epoch 100/100\n",
            "172/172 [==============================] - 23s 133ms/step - loss: 1.1288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHWAoXDB5Dhh",
        "colab_type": "text"
      },
      "source": [
        "# PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SifQ9NGR5Dhp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "87d38981-e901-4276-9f7c-884d5fccbde8"
      },
      "source": [
        "model = buildModel(vocabSize, embeddingDim, rnnUnits, batchSize=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaRlISFu5Dh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateText(model, startString):\n",
        "    numGen = 500\n",
        "    inputEval = [char2idx[s] for s in startString]\n",
        "    inputEval = tf.expand_dims(inputEval, 0)\n",
        "    \n",
        "    textGen = []\n",
        "    temperature = 1\n",
        "    \n",
        "    model.reset_states()\n",
        "    \n",
        "    for i in range(numGen):\n",
        "        predictions = model(inputEval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions /= temperature\n",
        "        predictedId = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        inputEval = tf.expand_dims([predictedId], 0)\n",
        "        textGen.append(idx2char[predictedId])\n",
        "        \n",
        "    return (startString + \"\".join(textGen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejkHZcoX5Diq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "9f70c547-ca36-40b8-d05d-0119dd500dcf"
      },
      "source": [
        "print(generateText(model, startString =\"ROMEO: \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: you are not to\n",
            "Cut a very eight:\n",
            "If I; But thou mean'd to pure clouds,\n",
            "But thou seest much shall be younger'd master. Put thy will\n",
            "that lay dream'd from that my red soot an old as delail.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "And how ear in love.\n",
            "\n",
            "PETER:\n",
            "Yourself, yet famous so indeed, sons, with the\n",
            "meal more than never should that Duke of her,\n",
            "Even posterse onea nighty self-father.\n",
            "\n",
            "Tell him for.\n",
            "\n",
            "JULIET:\n",
            "O, seeking council madam.\n",
            "\n",
            "BENRY BOLINGONT:\n",
            "Kind sir, news, have you been tick but off consent\n",
            "If no hate him \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbKEDp9tn42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}